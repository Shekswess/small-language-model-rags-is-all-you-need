experiment_name: "mixture-rag-llama3.1-8b-instruct-thought"

layers:
  - layer_type: "rag"
    layer_spec:
      - llm:
          provider: "groq"
          model_spec:
            model_name: "gemma2-9b-it"
            temperature: 0.1
            max_tokens: 4096
          prompt:
            system_message: | 
              You are an assistant that has a lot of knowledge about Large Language Models.
              Answer the user's question in a way that is easy to understand and informative.
              Use the provided context to generate a response that is relevant and accurate.
            user_message: "Please answer my question based on the provided context:"
      - llm:
          provider: "bedrock"
          model_spec:
            model_id: "mistral.mixtral-8x7b-instruct-v0:1"
            model_kwargs: 
              max_tokens: 4096
              temperature: 0.1
              top_p: 1
          prompt:
            system_message: | 
              You are an assistant that has a lot of knowledge about Large Language Models.
              Answer the user's question in a way that is easy to understand and informative.
              Use the provided context to generate a response that is relevant and accurate.
            user_message: "Please answer my question based on the provided context:"
      - llm:
          provider: "bedrock"
          model_spec:
            model_id: "anthropic.claude-3-haiku-20240307-v1:0"
            model_kwargs:
              max_tokens: 4096
              temperature: 0.1
              top_k: 250
              top_p: 1
              stop_sequences: ["\n\nHuman"]
          prompt:
            system_message: | 
              You are an assistant that has a lot of knowledge about Large Language Models.
              Answer the user's question in a way that is easy to understand and informative.
              Use the provided context to generate a response that is relevant and accurate.
            user_message: "Please answer my question based on the provided context:"
  - layer_type: "aggregator"
    layer_spec:
      - llm:
          provider: "bedrock"
          model_spec:
              model_id: "meta.llama3-1-8b-instruct-v1:0"
              model_kwargs: 
                max_tokens: 4096
                temperature: 0.1
                top_p: 1
          prompt:
            system_message: |
              You have been provided with a set of responses from various small language models to the latest user query. 
              The responses of the small language models are based on the context provided in the user query.
              Your task is to choose the best response from the provided responses.
              You should choose the response by analyzing all available responses and selecting the one you think is the most accurate and informative.
              Keep in mind the response must be a high-quality response, while getting the most faithful and relevant information from the provided responses.
              When you have made your choice, make that your final response and do not provide any additional responses, like explanations or clarifications why you chose that response.
            user_message: "Please choose a single response based on the provided responses:"

embedder:
  model_id: "amazon.titan-embed-text-v2:0"
  model_kwargs:
    dimensions: 512
    normalize: true

retriever:
  search_type: "similarity"
  retriever_kwargs:
    k: 5

chunker:
  chunk_size: 1500
  chunk_overlap: 100

data:
  path: "./data"

vector_store:
  path: "./data/database_1500_100"